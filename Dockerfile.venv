# Builder step used to download and configure spark environment
FROM ubuntu:20.04 as builder

# Install tzdata
RUN apt-get update && DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get -y install tzdata

# Add Dependencies for Python
RUN apt-get update && apt-get install -y software-properties-common wget libgomp1 build-essential

# Install java
RUN apt install -y default-jre

# Install python
RUN add-apt-repository ppa:deadsnakes/ppa
RUN apt install -y python3.9 python3.9-distutils
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1

# Install pip
RUN wget "https://bootstrap.pypa.io/get-pip.py" -O get-pip.py --no-check-certificate
RUN python3 get-pip.py

# Copy project files
COPY pyproject.toml /opt
COPY README.md /opt
COPY src /opt/src

# Install dependencies
RUN pip install --upgrade pip flit wheel setuptools
RUN export FLIT_ROOT_INSTALL=1 && cd /opt && flit install -s --deps production --extras testing,docs,local_development

# Fix the value of PYTHONHASHSEED
# Note: this is needed when you use Python 3.3 or greater
ENV SPARK_VERSION=3.5.0 \
HADOOP_VERSION=3 \
SPARK_HOME=/opt/spark \
PYTHONHASHSEED=1

# Download and uncompress spark from the apache archive
RUN wget --no-verbose -O apache-spark.tgz "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" \
&& mkdir -p /opt/spark \
&& tar -xf apache-spark.tgz -C /opt/spark --strip-components=1 \
&& rm apache-spark.tgz

# copy spark jars for interacting with S3 and database
COPY docker/drivers/postgresql-42.5.4.jar /root/.ivy2/jars/postgresql-42.5.4.jar
COPY docker/drivers/hadoop-common-3.3.4.jar /root/.ivy2/jars/hadoop-common-3.3.4.jar
COPY docker/drivers/hadoop-common-3.3.4.jar /root/.ivy2/jars/hadoop-common-3.3.4.jar
COPY docker/drivers/hadoop-aws-3.3.4.jar /root/.ivy2/jars/hadoop-aws-3.3.4.jar
RUN wget --no-verbose -O /root/.ivy2/jars/aws-java-sdk-bundle-1.12.594.jar "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.594/aws-java-sdk-bundle-1.12.594.jar"


# setup pyspark to run in jupyter
ENV PYSPARK_PYTHON=/usr/bin/python3 \
PYSPARK_DRIVER_PYTHON=/root/.local/bin/jupyter-notebook \
PATH=$PATH:/root/.local/bin
RUN /root/.local/bin/jupyter-notebook --generate-config \
&& echo "c.NotebookApp.allow_root=True" >> /root/.jupyter/jupyter_notebook_config.py \
&& echo "c.NotebookApp.ip='0.0.0.0'" >> /root/.jupyter/jupyter_notebook_config.py

# init airflow
ENV AIRFLOW__CORE__LOAD_EXAMPLES=False
RUN airflow db init
RUN airflow users create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin


# Apache spark environment
FROM builder as apache-spark

WORKDIR /opt